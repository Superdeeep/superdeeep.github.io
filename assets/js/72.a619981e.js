(window.webpackJsonp=window.webpackJsonp||[]).push([[72],{449:function(e,n,o){"use strict";o.r(n);var a=o(1),t=Object(a.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("本文主要以记录为主，所有的需要下载模型均可以在开源仓库找到，避免网盘进行传播（因为不知道第三方的上传者会不会加料）")]),e._v(" "),n("p",[e._v("本文参考 "),n("a",{attrs:{href:"https://github.com/ymcui/Chinese-LLaMA-Alpaca",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chinese-LLaMA-Alpaca"),n("OutboundLink")],1),e._v(" 文档以及 这篇博客 "),n("a",{attrs:{href:"https://blog.kala.love/posts/d1febb52/",target:"_blank",rel:"noopener noreferrer"}},[e._v("LLaMA大型语言模型的本地部署 >_<"),n("OutboundLink")],1),e._v(" 写成，并在本机上实测运行（Windows）")]),e._v(" "),n("p",[e._v("本文目标是部署本地的对话模型，因此使用Chinese-Alpaca-Plus-7B模型，和llama.cpp进行本地部署")]),e._v(" "),n("p",[e._v("本人还在学习如何使用GPU生成以及如何在此基础上训练自己的模型（还在学习中。。。）")]),e._v(" "),n("p",[e._v("本文可能存在一些疏漏，后续会不断完善。（🚧 施工中 🚧 ）")]),e._v(" "),n("p",[e._v("首先第一步下载原版的LLaMA模型  "),n("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-7b-hf/tree/main",target:"_blank",rel:"noopener noreferrer"}},[e._v("仓库地址"),n("OutboundLink")],1),e._v(" 由于此仓库已经换为HF格式了，因此直接拿来用就好了。")]),e._v(" "),n("p",[e._v("因此你的目录应该是这样的")]),e._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[e._v("│\n│\n└───llama-7b-hf  \n        .gitattributes\n        config.json\n        generation_config.json\n        LICENSE\n        pytorch_model-00001-of-00033.bin\n        pytorch_model-00002-of-00033.bin\n        pytorch_model-00003-of-00033.bin\n        pytorch_model-00004-of-00033.bin\n        pytorch_model-00005-of-00033.bin\n        pytorch_model-00006-of-00033.bin\n        pytorch_model-00007-of-00033.bin\n        pytorch_model-00008-of-00033.bin\n        pytorch_model-00009-of-00033.bin\n        pytorch_model-00010-of-00033.bin\n        pytorch_model-00011-of-00033.bin\n        pytorch_model-00012-of-00033.bin\n        pytorch_model-00013-of-00033.bin\n        pytorch_model-00014-of-00033.bin\n        pytorch_model-00015-of-00033.bin\n        pytorch_model-00016-of-00033.bin\n        pytorch_model-00017-of-00033.bin\n        pytorch_model-00018-of-00033.bin\n        pytorch_model-00019-of-00033.bin\n        pytorch_model-00020-of-00033.bin\n        pytorch_model-00021-of-00033.bin\n        pytorch_model-00022-of-00033.bin\n        pytorch_model-00023-of-00033.bin\n        pytorch_model-00024-of-00033.bin\n        pytorch_model-00025-of-00033.bin\n        pytorch_model-00026-of-00033.bin\n        pytorch_model-00027-of-00033.bin\n        pytorch_model-00028-of-00033.bin\n        pytorch_model-00029-of-00033.bin\n        pytorch_model-00030-of-00033.bin\n        pytorch_model-00031-of-00033.bin\n        pytorch_model-00032-of-00033.bin\n        pytorch_model-00033-of-00033.bin\n        pytorch_model.bin.index.json\n        README.md\n        special_tokens_map.json\n        tokenizer_config.json\n        tokenizer.model\n\n")])])]),n("p",[e._v("下载"),n("a",{attrs:{href:"https://drive.google.com/file/d/1N97m3rBj-rp-J1X8rgRfluyomEscfAq0/view",target:"_blank",rel:"noopener noreferrer"}},[e._v("chinese_llama_plus_lora_7b"),n("OutboundLink")],1),e._v("模型以及"),n("a",{attrs:{href:"https://drive.google.com/file/d/1EDcTmq6tDmRxqarpapdyDGBE9opY0zrB/view",target:"_blank",rel:"noopener noreferrer"}},[e._v("chinese_alpaca_plus_lora_7b"),n("OutboundLink")],1),e._v(" 模型")]),e._v(" "),n("p",[e._v("当然你可以在🤗Model Hub下载以上所有模型：")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://huggingface.co/ziqingyang/chinese-llama-plus-lora-7b",target:"_blank",rel:"noopener noreferrer"}},[e._v("ziqingyang/chinese-llama-plus-lora-7b"),n("OutboundLink")],1),e._v(" "),n("a",{attrs:{href:"https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-7b",target:"_blank",rel:"noopener noreferrer"}},[e._v("ziqingyang/chinese-alpaca-plus-lora-7b"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("他们的目录结构应该看起来像这样")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("│\n│\n└───chinese_llama_plus_lora_7b\n          adapter_config.json\n          adapter_model.bin\n          special_tokens_map.json\n          tokenizer_config.json\n          tokenizer.model      \n\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("│\n│\n└───chinese_llama_plus_lora_7b\n         adapter_config.json\n         adapter_model.bin\n         special_tokens_map.json\n         tokenizer_config.json\n         tokenizer.model\n         YOU_MUST_ALSO_DOWNLOAD_LLAMA_PLUS_7B.md   \n          \n")])])]),n("p",[e._v("接着进行模型的合并，为原版的模型添加中文的能力~")]),e._v(" "),n("p",[e._v("克隆仓库 https://github.com/ymcui/Chinese-LLaMA-Alpaca.git")]),e._v(" "),n("p",[e._v("进入 Chinese-LLaMA-Alpaca/scripts/ 目录下")]),e._v(" "),n("p",[e._v("找到"),n("a",{attrs:{href:"https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_llama_with_chinese_lora.py",target:"_blank",rel:"noopener noreferrer"}},[e._v("merge_llama_with_chinese_lora.py"),n("OutboundLink")],1),e._v("脚本")]),e._v(" "),n("p",[e._v("在此处打开终端执行命令")]),e._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[e._v("python merge_llama_with_chinese_lora.py "),n("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--base_model")]),e._v(" llama-7b-hf "),n("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--lora_model")]),e._v(" chinese_llama_plus_lora_7b,chinese_alpaca_plus_lora_7b "),n("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--output_dir")]),e._v(" out\n")])])]),n("p",[e._v("注意：此过程需要消耗15GB左右的内存（你没看错，是内存。）")]),e._v(" "),n("p",[e._v("合成后的目录应该像这样")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("│\n│\n└───out\n    config.json\n    consolidated.00.pth\n    generation_config.json\n    params.json\n    pytorch_model-00001-of-00002.bin\n    pytorch_model-00002-of-00002.bin\n    pytorch_model.bin.index.json\n    special_tokens_map.json\n    tokenizer_config.json\n    tokenizer.model\n")])])]),n("p",[e._v("之后就是需要量化模型")]),e._v(" "),n("p",[e._v("新建zh-models文件家夹，将Chinese-LLaMA-Alpaca文件夹中的tokenizer.model放入其中，然后在zh-models中建立7B文件夹，将上面合并生成的consolidated.00.pth和params.json放入其中")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("zh-models\n│   \n│    tokenizer.model\n|\n└───7B\n        consolidated.00.pth\n        params.json\n")])])]),n("p",[e._v("之后克隆仓库https://github.com/ggerganov/llama.cpp")]),e._v(" "),n("p",[e._v("找到 llama.cpp/convert-pth-to-ggml.py")]),e._v(" "),n("p",[e._v("执行命令"),n("code",[e._v("python convert-pth-to-ggml.py zh-models/7B/ 1")])]),e._v(" "),n("p",[e._v("你会在zh-models\\7B下得到一个ggml-model-f16.bin文件")]),e._v(" "),n("p",[e._v("现在我们需要对它Q4量化，这里需要用到llama.cpp不想编译的可以直接在https://github.com/ggerganov/llama.cpp/releases下载")]),e._v(" "),n("p",[e._v("我们需要quantize.exe进行量化模型")]),e._v(" "),n("p",[e._v("执行命令"),n("code",[e._v("./quantize.exe ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin 2")])]),e._v(" "),n("p",[e._v("最终你会得到ggml-model-q4_0.bin文件，这就是我们需要的了。")]),e._v(" "),n("p",[e._v("执行命令来启动"),n("code",[e._v("./main.exe -m zh-models/7B/ggml-model-q4_0.bin --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.3")])]),e._v(" "),n("p",[e._v("效果图：")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/Superdeeep/photo/llamahangzhouques.png",alt:"llamahangzhouques"}})])])}),[],!1,null,null,null);n.default=t.exports}}]);